# QUANDO É SEGURO USAR IA EXTERNA? + TÉCNICAS DE PROTEÇÃO (LGPD-Compliant)

> **"Dados reais? Só local. Dados seguros? Só externo."**

---

## QUANDO É SEGURO ENVIAR PARA IA EXTERNA?

| Situação | Seguro? | Por quê? |
|--------|--------|--------|
| **1. Dados 100% anonimizados ou sintéticos** | ✅ **SIM** | Sem PII, sem risco |
| **2. Consulta sem dados sensíveis/confidenciais** | ✅ **SIM** | Apenas lógica, sem informação real |
| **3. Prototipagem / Teste sem dados reais** | ✅ **SIM** | Dataset fictício = zero exposição |

> **Regra de Ouro**:  
> **Se não se encaixa em UM desses 3 → NÃO ENVIE.**

---

## 4 TÉCNICAS DE PROTEÇÃO (Escolha 1 ou mais)

| Técnica | Como funciona | Quando usar |
|--------|---------------|------------|
| **1. Tokenização** | Substitui dados reais por **tokens únicos** (ex: `user_123`) | Dados precisam ser rastreados internamente |
| **2. Mascaramento** | Esconde parte do dado (ex: `***.***.789-00`) | Dados precisam estar presentes, mas não legíveis |
| **3. Sandbox Interno (IA Privada)** | ChatGPT Enterprise, Azure OpenAI com contrato | Uso diário em empresa |
| **4. Consentimento Explícito** | Cliente **autoriza por escrito** o uso | Já previsto em termos de uso |

---

### 1. Tokenização (Rápida e Eficiente)

```python
# Antes
df['nome'] = 'Ana Silva'
df['cpf'] = '123.456.789-00'

# Depois (tokenizado)
df['cliente_token'] = ['token_001', 'token_002']
df = df.drop(columns=['nome', 'cpf'])
```

---

### 2. Mascaramento (Mantém estrutura, esconde valor)

| Original | Mascarado |
|--------|----------|
| `ana.silva@email.com` | `a**.s****@email.com` |
| `123.456.789-00` | `***.***.789-00` |
| `Rua das Flores, 123` | `Rua ***, **, **3` |

```python
df['email'] = df['email'].str[0] + '**' + df['email'].str[3:-10] + '@' + df['email'].str.split('@')[1]
df['cpf'] = '***.***.' + df['cpf'].str[-6:]
```

---

### 3. Sandbox Interno (Melhor para Empresas)

| Vantagem | Como funciona |
|--------|---------------|
| **Zero vazamento** | Dados NUNCA saem da rede |
| **Contrato DPA** | Provedor **não usa seus dados para treino** |
| **Liberdade total** | Time usa sem anonimizar todo dia |

> **Exemplos**:  
> - **ChatGPT Team / Enterprise**  
> - **Azure OpenAI com Private Endpoint**  
> - **Claude for Work**

---

### 4. Consentimento (Só se já estiver nos termos!)

> **NUNCA peça: "Posso vazar seu CPF no ChatGPT?"**

| Correto | Errado |
|--------|-------|
| Termos de uso do app:  
> "Seus dados serão processados por IA para melhorar experiência." | E-mail depois:  
> "Autoriza usar seu CPF no ChatGPT?" |

> **Atualização de termos?**  
> Cliente pode **recusar** → churn.

---

## CHECKLIST ANTES DE ENVIAR PARA IA EXTERNA

```markdown
- [ ] Dados são **100% anonimizados ou sintéticos**?
- [ ] Não tem **PII, financeiro, segredo comercial**?
- [ ] É apenas **prototipagem ou consulta lógica**?
- [ ] Usei **tokenização ou mascaramento** se necessário?
- [ ] Se for empresa: tem **sandbox interno**?
- [ ] Se for cliente: tem **consentimento nos termos**?
```

> **Se 1 item = NÃO → NÃO ENVIE.**

---

## PROMPT PADRÃO DE SEGURANÇA (Cole em TODO prompt)

> ```
> ANTES DE QUALQUER ANÁLISE:
> 1. **Confirme**: Dados são 100% anonimizados ou sintéticos?
> 2. **Tokenize ou mascare** qualquer resquício de PII.
> 3. **Use apenas dados fictícios** se for prototipagem.
> 4. **Não processe dados reais sem sandbox interno.**
> 5. **Responda apenas após confirmar**: "DADOS SEGUROS."
> ```

---

## PRÓXIMOS PASSOS (Ação IMEDIATA)

1. **Crie `dados_sinteticos.csv`** com 100 linhas fictícias.  
2. **Teste seu próximo prompt com dados falsos.**  
3. **Leia os termos de uso** da sua IA principal.  
4. **Propõe sandbox interno** no time (se aplicável).  
5. **Adicione o prompt de segurança** nos seus templates.

---

# CONCLUSÃO

> **IA externa é poderosa. Mas só com dados seguros.**

Você não precisa parar de usar IA.  
Você precisa **usar com inteligência**.

| Dados reais | → | **IA interna / sandbox** |
| Dados anonimizados/sintéticos | → | **IA externa** |

**Dados blindados. IA voando.**